{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rui\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = pickle.load(open('my_df_training_onehot.pickle', 'rb'))\n",
    "y = pickle.load(open('my_y.pickle', 'rb'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_training, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rui\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def tuned_clf(estimator,k,Xtrain,ytrain,pram_dist,ncv,njobs=1,scoring_method='None',**kwargs):\n",
    "    \"\"\"K: the number of features in SelectKBest method\n",
    "       Xtrain: training features\n",
    "       ytrain: traning labels \n",
    "       param dist: distribution parametes that are used in RandomizedSearchCV\n",
    "       ncv : number of cross-validation folds\n",
    "       This function will return the trained estimator\"\"\"\n",
    "    if kwargs:\n",
    "        clf = estimator(kwargs)\n",
    "    else:\n",
    "        clf = estimator\n",
    "    \n",
    "    if k == Xtrain.shape[1]:\n",
    "        pipe = make_pipeline(clf)\n",
    "    else:\n",
    "        pipe = make_pipeline(SelectKBest(k=k),clf)\n",
    "    \n",
    "    grid_clf = RandomizedSearchCV(pipe,param_distributions= param_dist,cv=ncv,n_jobs=njobs,scoring=scoring_method)\n",
    "    grid_clf.fit(Xtrain,ytrain) \n",
    "                                  \n",
    "    return  grid_clf\n",
    "\n",
    "def tuned_estimators(estimator,Xtrain,ytrain,Xtest,ytest,param_dist,n_features_list,ncv=5,njobs=1,scoring_method='None',\n",
    "                   verbose=False,**kwargs):\n",
    "    '''A kbest and a randomforestclassifier are embeded in a pipeline and a randomizedsearchCV tunes the\n",
    "     hyperparameters'''\n",
    "    models = defaultdict(str)\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    roc_auc_scores = []\n",
    "    nfeatures = []\n",
    "    for k in n_features_list:\n",
    "        nfeatures.append(k)\n",
    "        model_name = 'clf_k'+ str(k)\n",
    "        clf = tuned_clf(estimator,k,Xtrain,ytrain,param_dist,ncv,njobs,scoring_method,**kwargs)\n",
    "        models[model_name] = clf.best_estimator_\n",
    "        ypred = models[model_name].predict(Xtest)\n",
    "        accuracy_scores.append (accuracy_score(ypred,ytest))\n",
    "        f1_scores.append(f1_score(ypred,ytest))\n",
    "        roc_auc_scores.append(roc_auc_score(ypred,ytest))\n",
    "        if verbose:\n",
    "            print('%s best features: accuracy=%.4f, f1=%.4f, roc_auc=%.4f' % \n",
    "                  (k,accuracy_scores[-1],f1_scores[-1],roc_auc_scores[-1]))\n",
    "    plt.title('Effect of feature elimination on accuracy, f1, roc_auc scores')\n",
    "    plt.xlabel(\"K best features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(nfeatures)\n",
    "    plt.grid(b=True)\n",
    "    plt.plot(nfeatures,accuracy_scores,'o-', color=\"r\",label=\"accuracy score\")\n",
    "    plt.plot(nfeatures,f1_scores,'o-', color=\"b\",label=\"f1 score\")\n",
    "    plt.plot(nfeatures,roc_auc_scores,'o-', color=\"g\",label=\"roc_auc score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def Summary_Results(estimator,X_test,y_test):\n",
    "    ypred = estimator.predict(X_test)\n",
    "    print('The accuracy is: %.2f3 \\n' % accuracy_score(ypred,y_test))\n",
    "    print('Confusion_matrix:')\n",
    "    cm = confusion_matrix(y_test, ypred)\n",
    "    print('\\t\\t pridicted values')\n",
    "    print('\\t\\t 0 \\t 1')\n",
    "    print('actual 0: ','\\t',cm[0,0],'\\t',cm[0,1])\n",
    "    print('values 1: ','\\t',cm[1,0],'\\t',cm[1,1])\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Classification_report: \\n')\n",
    "    print(classification_report(y_test,ypred,target_names=[\"class 0\",\"class 1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_roc_curve(estimator,Xtest,ytest,figsize=(8,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    fpr_rf, tpr_rf, threshold = roc_curve(ytest,estimator.predict_proba(Xtest)[:,1])\n",
    "    plt.plot(fpr_rf, tpr_rf)\n",
    "    plt.xlabel('False positive rate',fontsize=16)\n",
    "    plt.ylabel('True positive rate',fontsize=16)\n",
    "    plt.title('ROC Curve',fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "sns.set_context('notebook',font_scale=1)\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, ylim=(0, 1.1), cv=5,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5),\n",
    "                        scoring=None):\n",
    "    plt.title(\"Learning curves for %s\" % type(estimator).__name__)\n",
    "    plt.ylim(*ylim); plt.grid()\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "        scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    \n",
    "    plt.grid(b=True)\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    print(\"Best validation score: {:.4f}\".format(validation_scores_mean[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.99      0.05      0.93      0.54      0.31     16015\n",
      "          1       0.33      0.05      0.99      0.09      0.54      0.28      2231\n",
      "\n",
      "avg / total       0.81      0.87      0.16      0.83      0.54      0.31     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.99      0.03      0.93      0.56      0.34     16015\n",
      "          1       0.36      0.03      0.99      0.06      0.56      0.30      2231\n",
      "\n",
      "avg / total       0.82      0.87      0.15      0.83      0.56      0.33     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      1.00      0.01      0.94      0.74      0.56     16015\n",
      "          1       0.62      0.01      1.00      0.03      0.74      0.53      2231\n",
      "\n",
      "avg / total       0.85      0.88      0.13      0.82      0.74      0.56     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import defaultdict\n",
    "\n",
    "estimators = {'RandomForest':RandomForestClassifier(),'AdaBoost': AdaBoostClassifier(), 'Extra Tree': ExtraTreesClassifier()}\n",
    "clfs = defaultdict(str)\n",
    "\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.09      0.93      0.52      0.28     16015\n",
      "          1       0.30      0.09      0.97      0.14      0.52      0.25      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.20      0.83      0.52      0.28     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.97      0.11      0.93      0.54      0.31     16015\n",
      "          1       0.33      0.11      0.97      0.16      0.54      0.27      2231\n",
      "\n",
      "avg / total       0.82      0.86      0.21      0.83      0.54      0.30     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.91      0.21      0.90      0.47      0.23     16015\n",
      "          1       0.24      0.21      0.91      0.23      0.47      0.20      2231\n",
      "\n",
      "avg / total       0.81      0.82      0.30      0.82      0.47      0.23     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oversample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sme = SMOTE()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTEENN_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.80      0.43      0.85      0.46      0.22     16015\n",
      "          1       0.23      0.43      0.80      0.30      0.46      0.20      2231\n",
      "\n",
      "avg / total       0.83      0.75      0.48      0.78      0.46      0.22     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.79      0.44      0.85      0.46      0.22     16015\n",
      "          1       0.23      0.44      0.79      0.30      0.46      0.19      2231\n",
      "\n",
      "avg / total       0.83      0.75      0.48      0.78      0.46      0.22     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.92      0.62      0.61      0.74      0.41      0.18     16015\n",
      "          1       0.18      0.61      0.62      0.28      0.41      0.16      2231\n",
      "\n",
      "avg / total       0.83      0.62      0.61      0.69      0.41      0.18     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine\n",
    "from imblearn.combine import SMOTEENN\n",
    "sme = SMOTEENN()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTEENN_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.08      0.93      0.51      0.27     16015\n",
      "          1       0.29      0.08      0.97      0.13      0.51      0.24      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.19      0.83      0.51      0.27     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.08      0.93      0.50      0.26     16015\n",
      "          1       0.28      0.08      0.97      0.12      0.50      0.23      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.19      0.83      0.50      0.26     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.91      0.23      0.90      0.48      0.24     16015\n",
      "          1       0.25      0.23      0.91      0.24      0.48      0.21      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.31      0.82      0.48      0.24     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine\n",
    "from imblearn.combine import SMOTETomek\n",
    "sme = SMOTETomek()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTETomek_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest_SMOTEENN_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_SMOTETomek_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_SMOTEENN_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "RandomForest_SMOTETomek_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_SMOTEENN_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "RandomForest_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_SMOTETomek_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "for name,clf in clfs.items():\n",
    "    print(name+':')\n",
    "    Summary_Results(clfs[name],X_test.as_matrix(),y_test.as_matrix())\n",
    "    print('================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "sme = SMOTEENN()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "y_pred = rbf_pca.predict(X_test)\n",
    "print(name + 'svm_clf')\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
